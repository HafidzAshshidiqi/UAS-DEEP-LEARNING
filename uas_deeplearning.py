# -*- coding: utf-8 -*-
"""UAS_DEEPLEARNING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ChmEpV7eV8EaDhuOSc90p_Mtg3bNOY1I

# NAMA KELOMPOK :
1. AKRAM ANALIS (G1A022004)
2. M. HAFIDZ ASHSHIDIQI (G1A022079)

# CNN-LSTM untuk Klasifikasi Anomali Data Sensor IoT

## 1. Import dan Instalasi Library
"""

!pip install kagglehub pandas numpy matplotlib scikit-learn

"""IMPORT LIBRARY"""

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

"""## 2. Load Dataset IoT Temperature

"""

import kagglehub
path = kagglehub.dataset_download("atulanandjha/temperature-readings-iot-devices")

print("Dataset path:", path)

data = pd.read_csv(os.path.join(path, 'IOT-temp.csv'))

print(data.head())

data.info()

"""## 3. Pra-pemrosesan Data

"""

data['noted_date'] = pd.to_datetime(
    data['noted_date'],
    dayfirst=True,
    errors='coerce'
)
data = data.sort_values('noted_date').reset_index(drop=True)

# Hapus baris tanggal yang gagal diparse (NaT)
data = data.dropna(subset=['noted_date'])

# Urutkan berdasarkan waktu
data = data.sort_values('noted_date').reset_index(drop=True)

data['noted_date'].head()
data['noted_date'].tail()

"""3.1 Pelabelan Anomali

"""

mean_temp = data['temp'].mean()
std_temp = data['temp'].std()

data['label'] = (data['temp'] > mean_temp + 2*std_temp).astype(int)

data[['temp', 'label']].head()

"""3.2 Visualisasi Data"""

plt.figure(figsize=(12,4))
plt.plot(data['temp'], label='Temperature')
plt.scatter(
    data.index[data['label']==1],
    data['temp'][data['label']==1],
    color='red',
    s=10,
    label='Anomaly'
)
plt.legend()
plt.title("IoT Temperature Anomaly Detection")
plt.show()

"""## 4. Normalisasi dan Windowing Data

"""

features = data[['temp']].values
labels = data['label'].values

scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

def create_sequences(X, y, window_size=30):
    X_seq, y_seq = [], []
    for i in range(len(X) - window_size):
        X_seq.append(X[i:i+window_size])
        y_seq.append(y[i+window_size])
    return np.array(X_seq), np.array(y_seq)

WINDOW_SIZE = 30

X_seq, y_seq = create_sequences(
    features_scaled,
    labels,
    WINDOW_SIZE
)

print(X_seq.shape, y_seq.shape)

"""## 5. Pembagian Data Train, Validation, dan Test

Train : 70%
Val   : 15%
Test  : 15%
"""

X_train, X_temp, y_train, y_temp = train_test_split(
    X_seq, y_seq, test_size=0.3, shuffle=False
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, shuffle=False
)

print('Train:', X_train.shape)
print('Validation:', X_val.shape)
print('Test:', X_test.shape)

"""## 6. MODEL UTAMA  CNN-LSTM

"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, LSTM,
    Dense, Dropout
)
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu',
           input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(pool_size=2),

    LSTM(64, return_sequences=False),
    Dropout(0.3),

    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

"""## 7. Training Model"""

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

"""## 8. Evaluasi Model CNN-LSTM

"""

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)


accuracy = accuracy_score(y_test, y_pred)
print('\nAkurasi Test:', accuracy)

print('\nClassification Report:')
print(classification_report(y_test, y_pred))

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
)

y_pred_prob = model.predict(X_test).ravel()
y_pred = (y_pred_prob > 0.5).astype(int)

print("CNN-LSTM Accuracy :", accuracy_score(y_test, y_pred))
print("CNN-LSTM Precision:", precision_score(y_test, y_pred))
print("CNN-LSTM Recall   :", recall_score(y_test, y_pred))
print("CNN-LSTM F1-score :", f1_score(y_test, y_pred))
print("CNN-LSTM ROC-AUC  :", roc_auc_score(y_test, y_pred_prob))

plt.figure(figsize=(8,4))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

"""Grafik pelatihan menunjukkan bahwa nilai loss menurun secara konsisten
seiring bertambahnya epoch, sementara nilai accuracy meningkat dan stabil.
Hal ini mengindikasikan bahwa model CNN-LSTM mampu melakukan pembelajaran
dengan baik tanpa mengalami overfitting yang signifikan.

"""

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()
plt.title("Confusion Matrix - CNN LSTM")
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve

# Hitung ROC-AUC
roc_auc = roc_auc_score(y_test, y_pred_prob)

# Hitung ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Plot ROC Curve
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label=f'CNN-LSTM (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve CNN-LSTM')
plt.legend()
plt.grid(True)
plt.show()

print('ROC-AUC CNN-LSTM:', roc_auc)

"""## 9. Perbandingan dengan Baseline Model
(SVM & Autoencoder)

## BASELINE 1 — SVM
"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(
    features_scaled, labels,
    test_size=0.3,
    shuffle=False
)

svm_model = SVC(
    kernel='rbf',
    probability=True,
    class_weight='balanced'
)

svm_model.fit(X_train_svm, y_train_svm)

y_pred_svm = svm_model.predict(X_test_svm)
y_prob_svm = svm_model.predict_proba(X_test_svm)[:, 1]

print("SVM Accuracy :", accuracy_score(y_test_svm, y_pred_svm))
print("SVM Precision:", precision_score(y_test_svm, y_pred_svm))
print("SVM Recall   :", recall_score(y_test_svm, y_pred_svm))
print("SVM F1-score :", f1_score(y_test_svm, y_pred_svm))
print("SVM ROC-AUC  :", roc_auc_score(y_test_svm, y_prob_svm))

print('=== BASELINE SVM ===')
print('Accuracy:', accuracy_score(y_test_svm,  y_pred_svm))
print('ROC-AUC:', roc_auc_score(y_test_svm, y_prob_svm))
print(classification_report(y_test_svm, y_pred_svm))

ConfusionMatrixDisplay(
    confusion_matrix(y_test_svm, y_pred_svm)
).plot()
plt.title("Confusion Matrix - SVM")
plt.show()

"""## BASELINE 2 — AUTOENCODER"""

X_train_ae = X_train[y_train == 0]
X_test_ae = X_test

from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

input_dim = X_train_ae.shape[1] * X_train_ae.shape[2]

X_train_flat = X_train_ae.reshape(len(X_train_ae), -1)
X_test_flat = X_test_ae.reshape(len(X_test_ae), -1)

input_layer = Input(shape=(input_dim,))
encoded = Dense(64, activation='relu')(input_layer)
encoded = Dense(32, activation='relu')(encoded)
decoded = Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

autoencoder.fit(
    X_train_flat, X_train_flat,
    epochs=50,
    batch_size=64,
    validation_split=0.1,
    verbose=1
)

reconstructions = autoencoder.predict(X_test_flat)
mse = np.mean(np.power(X_test_flat - reconstructions, 2), axis=1)

threshold = np.percentile(mse, 95)
y_pred_ae = (mse > threshold).astype(int)

print("Autoencoder Accuracy :", accuracy_score(y_test, y_pred_ae))
print("Autoencoder Precision:", precision_score(y_test, y_pred_ae))
print("Autoencoder Recall   :", recall_score(y_test, y_pred_ae))
print("Autoencoder F1-score :", f1_score(y_test, y_pred_ae))
print("Autoencoder ROC-AUC  :", roc_auc_score(y_test, mse))

print('=== BASELINE AUTOENCODER ===')
print('Accuracy:', accuracy_score(y_test, y_pred_ae))
print('ROC-AUC:', roc_auc_score(y_test, mse))
print(classification_report(y_test, y_pred_ae))

ConfusionMatrixDisplay(
    confusion_matrix(y_test, y_pred_ae)
).plot()
plt.title("Confusion Matrix - Autoencoder")
plt.show()

"""## Kesimpulan

Berdasarkan hasil pengujian pada data uji, model CNN-LSTM yang diusulkan
mampu mencapai nilai akurasi sebesar 99,37% dan ROC-AUC sebesar 0,983.
Nilai ROC-AUC yang mendekati 1 menunjukkan bahwa model memiliki kemampuan
yang sangat baik dalam membedakan antara data suhu normal dan data anomali
pada berbagai ambang keputusan.

Namun demikian, hasil evaluasi juga menunjukkan bahwa nilai recall untuk
kelas anomali masih relatif rendah, yaitu sebesar 0,20, dengan nilai
precision sebesar 0,61 dan F1-score sebesar 0,31. Hal ini disebabkan oleh
ketidakseimbangan jumlah data, di mana jumlah data normal jauh lebih besar
dibandingkan data anomali. Kondisi ini menyebabkan model cenderung lebih
fokus dalam mengenali pola data normal dibandingkan mendeteksi seluruh
anomali yang ada.

Meskipun demikian, performa ROC-AUC yang tinggi membuktikan bahwa model
CNN-LSTM tetap mampu mempelajari pola temporal pada data suhu IoT dengan
baik. Dibandingkan dengan metode baseline, pendekatan hybrid CNN-LSTM
menunjukkan potensi yang lebih unggul dalam menangani data time-series
berbasis Internet of Things.

Dengan demikian, model CNN-LSTM dapat dijadikan sebagai dasar sistem
deteksi anomali suhu IoT, namun diperlukan pengembangan lebih lanjut,
seperti penanganan data tidak seimbang atau penyesuaian threshold, untuk
meningkatkan kemampuan deteksi anomali secara menyeluruh.

| Metrik              | Nilai      |
| ------------------- | ---------- |
| Accuracy            | **99.37%** |
| Precision (Anomali) | **0.61**   |
| Recall (Anomali)    | **0.20**   |
| F1-score            | **0.31**   |
| ROC-AUC             | **0.983**  |


Nilai ROC-AUC yang tinggi menunjukkan bahwa model masih memiliki potensi
yang baik apabila dilakukan penyesuaian threshold atau teknik penanganan
data tidak seimbang, seperti oversampling atau cost-sensitive learning.

"""

model.save('cnn_lstm_iot_model.h5')
print('Model CNN-LSTM berhasil disimpan.')

"""# 10. OPTIMASI & DEPLOYMENT MODEL

1. OPTIMASI MODEL KERAS
"""

!pip install keras-tuner

import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout

def build_model(hp):
    model = Sequential()

    model.add(
        Conv1D(
            filters=hp.Int('filters', 32, 128, step=32),
            kernel_size=hp.Choice('kernel_size', [3,5]),
            activation='relu',
            input_shape=(X_train.shape[1], X_train.shape[2])
        )
    )
    model.add(MaxPooling1D(2))

    model.add(
        LSTM(
            units=hp.Int('lstm_units', 32, 128, step=32)
        )
    )

    model.add(
        Dropout(
            hp.Float('dropout', 0.2, 0.5, step=0.1)
        )
    )

    model.add(Dense(1, activation='sigmoid'))

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    directory='tuning',
    project_name='cnn_lstm_iot'
)

tuner.search(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=64
)

best_model = tuner.get_best_models(num_models=1)[0]
best_model.summary()

"""2. OPTIMASI MODEL TENSORFLOW LITE"""

converter = tf.lite.TFLiteConverter.from_keras_model(best_model)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]

# Matikan lowering TensorList
converter._experimental_lower_tensor_list_ops = False

tflite_model = converter.convert()

with open("cnn_lstm_iot.tflite", "wb") as f:
    f.write(tflite_model)

print("Model berhasil dikonversi ke TensorFlow Lite")

"""# DEMO SINGKAT"""

interpreter = tf.lite.Interpreter(model_path='cnn_lstm_iot.tflite')
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

sample = X_test[0:1].astype(np.float32)
interpreter.set_tensor(input_details[0]['index'], sample)
interpreter.invoke()

prediction = interpreter.get_tensor(output_details[0]['index'])
print("Prediksi Anomali:", prediction)

sample = X_test[10:11]
pred = model.predict(sample)
print("Probabilitas Anomali:", pred)

threshold = 0.5
label = "Anomali" if prediction[0][0] > threshold else "Normal"
print("Status:", label)

"""# DEMO SINGKAT"""

# Ambil satu sample dari data uji
sample = X_test[0:1].astype(np.float32)

print("Input shape:", sample.shape)
print("Contoh data suhu (window):")
print(sample.reshape(-1))

keras_pred = model.predict(sample)
print("Probabilitas Anomali (Keras):", keras_pred[0][0])

interpreter = tf.lite.Interpreter(model_path="cnn_lstm_iot.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], sample)
interpreter.invoke()

tflite_pred = interpreter.get_tensor(output_details[0]['index'])
print("Probabilitas Anomali (TFLite):", tflite_pred[0][0])

threshold = 0.5

status = "Anomali" if tflite_pred[0][0] > threshold else "Normal"
print("Status Deteksi:", status)